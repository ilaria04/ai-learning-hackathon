{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:08:58.194045Z",
     "start_time": "2024-12-07T19:08:57.436470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Load dlib's pre-trained face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Get facial landmark indices for the eyes\n",
    "LEFT_EYE_POINTS = list(range(36, 42))\n",
    "RIGHT_EYE_POINTS = list(range(42, 48))\n",
    "\n",
    "# Microsaccade parameters\n",
    "MICROSACCADE_VELOCITY_THRESHOLD = 20  # Velocity threshold in pixels/second\n",
    "SUSTAINED_TIME_THRESHOLD = 5  # Time in seconds\n",
    "FRAME_WINDOW_SIZE = 5  # Number of frames in the 5-second window\n",
    "FRAME_THRESHOLD = 0.8  # 80% of frames should exceed the threshold\n",
    "\n",
    "# Function to calculate midpoint between two points\n",
    "def midpoint(point1, point2):\n",
    "    return int((point1.x + point2.x) / 2), int((point1.y + point2.y) / 2)\n",
    "\n",
    "# Function to determine the gaze direction (screen or outside)\n",
    "def get_gaze_ratio(eye_points, facial_landmarks, frame):\n",
    "    eye_region = np.array([(facial_landmarks.part(point).x, facial_landmarks.part(point).y) for point in eye_points])\n",
    "    height, width, _ = frame.shape\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.polylines(mask, [eye_region], isClosed=True, color=255, thickness=2)\n",
    "    cv2.fillPoly(mask, [eye_region], color=255)\n",
    "\n",
    "    eye_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    min_x = np.min(eye_region[:, 0])\n",
    "    max_x = np.max(eye_region[:, 0])\n",
    "    min_y = np.min(eye_region[:, 1])\n",
    "    max_y = np.max(eye_region[:, 1])\n",
    "\n",
    "    gray_eye = cv2.cvtColor(eye_frame[min_y:max_y, min_x:max_x], cv2.COLOR_BGR2GRAY)\n",
    "    _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, _ = cv2.findContours(threshold_eye, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        M = cv2.moments(max_contour)\n",
    "\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])  # Centroid of the contour (pupil)\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "            cv2.circle(eye_frame, (min_x + cx, min_y + cy), 4, (0, 0, 255), 2)\n",
    "\n",
    "            eye_width = max_x - min_x\n",
    "            gaze_ratio = cx / eye_width  # Ratio of where the pupil is located\n",
    "\n",
    "            return gaze_ratio\n",
    "    return None\n",
    "\n",
    "# Function to determine pupil position\n",
    "def get_pupil_position(eye_points, facial_landmarks, frame):\n",
    "    eye_region = np.array([(facial_landmarks.part(point).x, facial_landmarks.part(point).y) for point in eye_points])\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.polylines(mask, [eye_region], isClosed=True, color=255, thickness=2)\n",
    "    cv2.fillPoly(mask, [eye_region], color=255)\n",
    "\n",
    "    eye_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    min_x = np.min(eye_region[:, 0])\n",
    "    max_x = np.max(eye_region[:, 0])\n",
    "    min_y = np.min(eye_region[:, 1])\n",
    "    max_y = np.max(eye_region[:, 1])\n",
    "\n",
    "    gray_eye = cv2.cvtColor(eye_frame[min_y:max_y, min_x:max_x], cv2.COLOR_BGR2GRAY)\n",
    "    _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, _ = cv2.findContours(threshold_eye, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        M = cv2.moments(max_contour)\n",
    "\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])  # Centroid of the contour (pupil)\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "            cv2.circle(eye_frame, (min_x + cx, min_y + cy), 4, (0, 0, 255), 2)\n",
    "            return (min_x + cx, min_y + cy)  # Return the absolute position of the pupil\n",
    "\n",
    "    return None\n",
    "\n",
    "# Main function to detect both microsaccades and inattention\n",
    "def detect_microsaccades_and_inattention(video_source=1):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    prev_left_pupil = None\n",
    "    prev_right_pupil = None\n",
    "    prev_time = time.time()\n",
    "\n",
    "    left_velocity_window = deque(maxlen=FRAME_WINDOW_SIZE)\n",
    "    right_velocity_window = deque(maxlen=FRAME_WINDOW_SIZE)\n",
    "\n",
    "    # Variables to track inattention time\n",
    "    inattention_frames = 0\n",
    "    total_frames = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Microsaccade detection logic\n",
    "            left_pupil = get_pupil_position(LEFT_EYE_POINTS, landmarks, frame)\n",
    "            right_pupil = get_pupil_position(RIGHT_EYE_POINTS, landmarks, frame)\n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - prev_time\n",
    "\n",
    "            if left_pupil and right_pupil and time_elapsed > 0:\n",
    "                if prev_left_pupil and prev_right_pupil:\n",
    "                    left_movement = np.linalg.norm(np.array(left_pupil) - np.array(prev_left_pupil))\n",
    "                    right_movement = np.linalg.norm(np.array(right_pupil) - np.array(prev_right_pupil))\n",
    "\n",
    "                    left_velocity = left_movement / time_elapsed\n",
    "                    right_velocity = right_movement / time_elapsed\n",
    "\n",
    "                    left_velocity_window.append(left_velocity)\n",
    "                    right_velocity_window.append(right_velocity)\n",
    "\n",
    "                    left_above_threshold = sum(1 for v in left_velocity_window if v > MICROSACCADE_VELOCITY_THRESHOLD) / len(left_velocity_window)\n",
    "                    right_above_threshold = sum(1 for v in right_velocity_window if v > MICROSACCADE_VELOCITY_THRESHOLD) / len(right_velocity_window)\n",
    "\n",
    "                    if left_above_threshold >= FRAME_THRESHOLD and right_above_threshold >= FRAME_THRESHOLD:\n",
    "                        print(f\"Sustained microsaccade detected!\")\n",
    "                        \n",
    "                        video_player.pause()\n",
    "                    \n",
    "                        # Show the message\n",
    "                        display_message_on_player(\"Microsaccade detected!\")\n",
    "\n",
    "                prev_left_pupil = left_pupil\n",
    "                prev_right_pupil = right_pupil\n",
    "                prev_time = current_time\n",
    "\n",
    "            # Inattention detection logic\n",
    "            left_gaze_ratio = get_gaze_ratio(LEFT_EYE_POINTS, landmarks, frame)\n",
    "            right_gaze_ratio = get_gaze_ratio(RIGHT_EYE_POINTS, landmarks, frame)\n",
    "\n",
    "            if left_gaze_ratio is not None and right_gaze_ratio is not None:\n",
    "                gaze_ratio = (left_gaze_ratio + right_gaze_ratio) / 2.0\n",
    "\n",
    "                if gaze_ratio < 0.4 or gaze_ratio > 0.6:\n",
    "                    cv2.putText(frame, \"Inattentive\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    inattention_frames += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Attentive\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                total_frames += 1\n",
    "\n",
    "                current_time = time.time()\n",
    "                if current_time - start_time >= 5:\n",
    "                    if (inattention_frames / total_frames) >= 0.8:\n",
    "                        print(\"Alert: Student has been inattentive for 5 seconds!\")\n",
    "                        cv2.putText(frame, \"ALERT: Inattentive!\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    inattention_frames = 0\n",
    "                    total_frames = 0\n",
    "\n",
    "        cv2.imshow(\"Microsaccade and Inattention Detector\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "fb1b69e6b80a5f3a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-07T19:08:58.960104Z"
    }
   },
   "source": [
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import threading\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QWidget, QLabel, QInputDialog, QMessageBox\n",
    "from PyQt5.QtWebEngineWidgets import QWebEngineView, QWebEnginePage\n",
    "from PyQt5.QtCore import QUrl, QTimer, pyqtSignal, QObject\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Define a signal class for thread-safe communication between the detector and UI\n",
    "class Communicate(QObject):\n",
    "    update_signal = pyqtSignal(str)\n",
    "\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "class YouTubePlayer(QMainWindow):\n",
    "    def __init__(self, video_url):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle('YouTube Video Player')\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        # Create a widget to hold the video\n",
    "        widget = QWidget(self)\n",
    "        self.setCentralWidget(widget)\n",
    "\n",
    "        # Create a layout to embed the video player\n",
    "        layout = QVBoxLayout()\n",
    "        widget.setLayout(layout)\n",
    "\n",
    "        # Create a QWebEngineView to display the YouTube video\n",
    "        self.browser = QWebEngineView()\n",
    "        layout.addWidget(self.browser)\n",
    "\n",
    "        # Embed the YouTube video using the video URL\n",
    "        embed_url = f\"https://www.youtube.com/embed/{self.extract_video_id(video_url)}\"\n",
    "        self.browser.setUrl(QUrl(embed_url))\n",
    "\n",
    "        # Initialize the communicator\n",
    "        self.comm = Communicate()\n",
    "        self.comm.update_signal.connect(self.update_status)\n",
    "        \n",
    "        self.message_label = QLabel(self)\n",
    "        self.message_label.setStyleSheet(\"background-color: red; color: white; font-size: 18px; padding: 10px;\")\n",
    "        self.message_label.setText(\"Microsaccade detected!\")\n",
    "        self.message_label.setVisible(False)  # Initially hidden\n",
    "        layout.addWidget(self.message_label)\n",
    "        \n",
    "        # Start the microsaccade detection in a separate thread\n",
    "        self.detection_thread = threading.Thread(target=self.detect_microsaccades_and_inattention)\n",
    "        self.detection_thread.daemon = True  # Daemonize the thread so it exits with the main program\n",
    "        self.detection_thread.start()\n",
    "\n",
    "    def extract_video_id(self, url):\n",
    "        \"\"\"Extract the YouTube video ID from the given URL.\"\"\"\n",
    "        if \"youtube.com\" in url:\n",
    "            # Extract the video ID from a youtube URL\n",
    "            video_id = url.split('v=')[1].split('&')[0]\n",
    "            return video_id\n",
    "        elif \"youtu.be\" in url:\n",
    "            # Extract the video ID from a youtu.be URL\n",
    "            return url.split('/')[-1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid YouTube URL\")\n",
    "\n",
    "    def update_status(self, message):\n",
    "        \"\"\"Update the UI with the detection status (e.g., inattention).\"\"\"\n",
    "        print(message)\n",
    "        if message == \"microsaccade\":\n",
    "            self.pause_video()\n",
    "            self.message_label.setText(\"Microsaccade detected!\")\n",
    "            self.message_label.setVisible(True)\n",
    "        elif message == \"resume\":\n",
    "            self.resume_video()\n",
    "            self.message_label.setVisible(False)\n",
    "        elif message == \"inattention\":\n",
    "            self.pause_video()\n",
    "            self.prompt_summary()\n",
    "            # self.message_label.setText(\"Inattention detected for 5 seconds!\")\n",
    "            # self.message_label.setVisible(True)\n",
    "\n",
    "    def pause_video(self):\n",
    "        \"\"\"Inject JavaScript to pause the video.\"\"\"\n",
    "        self.browser.page().runJavaScript(\"document.querySelector('video').pause();\")\n",
    "\n",
    "    def resume_video(self):\n",
    "        \"\"\"Inject JavaScript to resume the video.\"\"\"\n",
    "        self.browser.page().runJavaScript(\"document.querySelector('video').play();\")\n",
    "        \n",
    "    def prompt_summary(self):\n",
    "        \"\"\"Prompt the student to enter a summary of their learnings.\"\"\"\n",
    "        conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are speaking to a student who is distracted while watching a video. You just prompted the student to enter a summary of his/her learnings. React in an interested manner and ask probing, reflective questions on the response to engage the student further.\"}\n",
    "        ]\n",
    "        \n",
    "        # Initial prompt to the student\n",
    "        text, ok = QInputDialog.getText(self, \"Sustained Inattention\", \n",
    "                                        \"You've been inattentive for a while.\\nPlease summarize what you've learned so far:\")\n",
    "\n",
    "        if ok and text:\n",
    "            count = 1\n",
    "            while count < 4:\n",
    "                conversation_history.append({\"role\": \"user\", \"content\": text})\n",
    "                data = {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"messages\": conversation_history,\n",
    "                \"temperature\": 0.7\n",
    "                }\n",
    "                response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "                chatbot_reply = response.json().get(\"choices\")[0][\"message\"][\"content\"]\n",
    "                conversation_history.append({\"role\": \"system\", \"content\": chatbot_reply})\n",
    "                text, ok = QInputDialog.getText(self,\"Sustained Inattention\",chatbot_reply)\n",
    "                count += 1\n",
    "                # # Display a message box with the entered summary\n",
    "                # QMessageBox.information(self, \"Summary\", f\"Your summary:\\n{text}\")\n",
    "            # You can store the summary or process it further here\n",
    "        else:\n",
    "            QMessageBox.warning(self, \"No Summary\", \"Please pay attention to the video.\")\n",
    "        \n",
    "        temp = conversation_history[-1][\"content\"] + \"Now summarize the entire conversation in a brief paragraph and provide encouragement to the student.\"\n",
    "        conversation_history[-1]['content'] = temp\n",
    "        data = {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"messages\": conversation_history,\n",
    "                \"temperature\": 0.7\n",
    "                }\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        summary = response.json().get(\"choices\")[0][\"message\"][\"content\"]\n",
    "        QMessageBox.information(self, \"Summary\", f\"Your summary:\\n{summary}\")\n",
    "            \n",
    "    def detect_microsaccades_and_inattention(self, video_source=1):\n",
    "        cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "        prev_left_pupil = None\n",
    "        prev_right_pupil = None\n",
    "        prev_time = time.time()\n",
    "\n",
    "        left_velocity_window = deque(maxlen=5)\n",
    "        right_velocity_window = deque(maxlen=5)\n",
    "\n",
    "        # Variables to track inattention time\n",
    "        inattention_frames = 0\n",
    "        total_frames = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            detector = dlib.get_frontal_face_detector()\n",
    "            faces = detector(gray)\n",
    "            \n",
    "            for face in faces:\n",
    "                landmarks = predictor(gray, face)\n",
    "\n",
    "                # Microsaccade detection logic\n",
    "                left_pupil = get_pupil_position(LEFT_EYE_POINTS, landmarks, frame)\n",
    "                right_pupil = get_pupil_position(RIGHT_EYE_POINTS, landmarks, frame)\n",
    "                current_time = time.time()\n",
    "                time_elapsed = current_time - prev_time\n",
    "    \n",
    "                if left_pupil and right_pupil and time_elapsed > 0:\n",
    "                    if prev_left_pupil and prev_right_pupil:\n",
    "                        left_movement = np.linalg.norm(np.array(left_pupil) - np.array(prev_left_pupil))\n",
    "                        right_movement = np.linalg.norm(np.array(right_pupil) - np.array(prev_right_pupil))\n",
    "    \n",
    "                        left_velocity = left_movement / time_elapsed\n",
    "                        right_velocity = right_movement / time_elapsed\n",
    "    \n",
    "                        left_velocity_window.append(left_velocity)\n",
    "                        right_velocity_window.append(right_velocity)\n",
    "    \n",
    "                        left_above_threshold = sum(1 for v in left_velocity_window if v > MICROSACCADE_VELOCITY_THRESHOLD) / len(left_velocity_window)\n",
    "                        right_above_threshold = sum(1 for v in right_velocity_window if v > MICROSACCADE_VELOCITY_THRESHOLD) / len(right_velocity_window)\n",
    "    \n",
    "                        if left_above_threshold >= FRAME_THRESHOLD and right_above_threshold >= FRAME_THRESHOLD:\n",
    "                            print(f\"Sustained microsaccade detected!\")\n",
    "                            self.comm.update_signal.emit(\"microsaccade\")\n",
    "                            time.sleep(3)  # Pause for 3 seconds\n",
    "                            self.comm.update_signal.emit(\"resume\")\n",
    "    \n",
    "                    prev_left_pupil = left_pupil\n",
    "                    prev_right_pupil = right_pupil\n",
    "                    prev_time = current_time\n",
    "    \n",
    "                # Inattention detection logic\n",
    "                left_gaze_ratio = get_gaze_ratio(LEFT_EYE_POINTS, landmarks, frame)\n",
    "                right_gaze_ratio = get_gaze_ratio(RIGHT_EYE_POINTS, landmarks, frame)\n",
    "    \n",
    "                if left_gaze_ratio is not None and right_gaze_ratio is not None:\n",
    "                    gaze_ratio = (left_gaze_ratio + right_gaze_ratio) / 2.0\n",
    "    \n",
    "                    if gaze_ratio < 0.4 or gaze_ratio > 0.6:\n",
    "                        cv2.putText(frame, \"Inattentive\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        inattention_frames += 1\n",
    "                    else:\n",
    "                        cv2.putText(frame, \"Attentive\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "                    total_frames += 1\n",
    "    \n",
    "                    current_time = time.time()\n",
    "                    if current_time - start_time >= 5:\n",
    "                        if (inattention_frames / total_frames) >= 0.8:\n",
    "                            print(\"Alert: Student has been inattentive for 5 seconds!\")\n",
    "                            self.comm.update_signal.emit(\"inattention\")\n",
    "                            time.sleep(3)  # Pause for 3 seconds\n",
    "                            self.comm.update_signal.emit(\"resume\")\n",
    "    \n",
    "                        start_time = time.time()\n",
    "                        inattention_frames = 0\n",
    "                        total_frames = 0\n",
    "\n",
    "            time.sleep(0.1)  # Optional: Add a small delay to prevent maxing out CPU\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    video_url = \"https://www.youtube.com/watch?v=FWTNMzK9vG4\"\n",
    "    window = YouTubePlayer(video_url)\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PrakharGoel/Desktop/hackathon/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d58f22ed794471c7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
